import requests
from bs4 import BeautifulSoup
import time
import re
import os
from urllib.parse import urljoin
import json
from pathlib import Path

class HaiziFullScraper:
    def __init__(self, base_url="https://haizi.huhaitai.com/"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
        self.output_dir = Path("../assets/content/poems")
        self.sections = {
            'preface': self.output_dir / '00-preface',
            'part1': self.output_dir / '01-short-poems-1983-1986', 
            'part2': self.output_dir / '02-long-poems-1984-1985',
            'part3': self.output_dir / '03-short-poems-1987-1989'
        }
        
        # åˆ›å»ºç›®å½•
        for section_dir in self.sections.values():
            section_dir.mkdir(parents=True, exist_ok=True)
        
        self.scraped_poems = []
        self.failed_poems = []
    
    def get_page(self, url):
        """è·å–é¡µé¢å†…å®¹"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return BeautifulSoup(response.text, 'html.parser')
        except Exception as e:
            print(f"âŒ è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None
    
    def extract_all_poem_links(self):
        """ä»ä¸»é¡µæå–æ‰€æœ‰è¯—æ­Œé“¾æ¥"""
        print("ğŸ“– æ­£åœ¨è·å–æ‰€æœ‰è¯—æ­Œé“¾æ¥...")
        soup = self.get_page(self.base_url)
        if not soup:
            return []
        
        links = []
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            title = link.get_text().strip()
            
            if (href and href.startswith('/') and 
                title and len(title) > 0 and
                not any(skip in title for skip in ['ç›®å½•', 'æ‰‰é¡µ', 'éª†ä¸€ç¦¾', 'è¥¿å·'])):
                
                full_url = urljoin(self.base_url, href)
                links.append({
                    'title': title,
                    'url': full_url,
                    'path': href,
                    'section': self.determine_section(href)
                })
        
        print(f"âœ… æ‰¾åˆ° {len(links)} é¦–è¯—æ­Œ")
        return links
    
    def determine_section(self, path):
        """æ ¹æ®è·¯å¾„ç¡®å®šè¯—æ­Œæ‰€å±ç« èŠ‚"""
        if path.startswith('/f'):
            return 'preface'
        elif path.startswith('/01/'):
            return 'part1'
        elif path.startswith('/02/'):
            return 'part2'
        elif path.startswith('/03/'):
            return 'part3'
        else:
            return 'part1'  # é»˜è®¤å½’ç±»
    
    def clean_filename(self, title):
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦"""
        # ç§»é™¤æˆ–æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
        filename = re.sub(r'[<>:"/\\|?*]', '', title)
        filename = re.sub(r'[ï¼Œã€‚ï¼šï¼›ï¼ï¼Ÿï¼ˆï¼‰ã€ã€‘ã€Šã€‹""''ã€]', '', filename)
        filename = filename.replace(' ', '-').replace('ã€€', '-')
        filename = re.sub(r'-+', '-', filename)  # åˆå¹¶å¤šä¸ªè¿å­—ç¬¦
        filename = filename.strip('-')
        return filename[:50]  # é™åˆ¶æ–‡ä»¶åé•¿åº¦
    
    def extract_poem_content(self, url, title):
        """æå–å•é¦–è¯—çš„è¯¦ç»†å†…å®¹"""
        soup = self.get_page(url)
        if not soup:
            return None
        
        # ç§»é™¤å¯¼èˆªå’Œè„šæœ¬å…ƒç´ 
        for tag in soup(['script', 'style', 'nav', 'header', 'footer']):
            tag.decompose()
        
        # è·å–ä¸»è¦å†…å®¹
        content_text = ""
        body = soup.find('body')
        if body:
            content_text = body.get_text(separator='\n')
        
        # æ¸…ç†å’Œæ ¼å¼åŒ–å†…å®¹
        lines = [line.strip() for line in content_text.split('\n') if line.strip()]
        
        # è¿‡æ»¤æ‰å¯¼èˆªå’Œæ— å…³å†…å®¹
        skip_patterns = [
            r'ç›®å½•', r'ç¬¬[ä¸€äºŒä¸‰]ç¼–', r'ä¸Šä¸€é¡µ', r'ä¸‹ä¸€é¡µ', r'è¿”å›',
            r'haizi\.huhaitai\.com', r'^\d+$', r'^[â†â†’]$'
        ]
        
        filtered_lines = []
        for line in lines:
            if (len(line) > 1 and 
                not any(re.search(pattern, line) for pattern in skip_patterns)):
                filtered_lines.append(line)
        
        # æå–è¯—æ­Œæ­£æ–‡ï¼ˆé€šå¸¸æ ‡é¢˜åé¢å°±æ˜¯æ­£æ–‡ï¼‰
        poem_content = []
        found_title = False
        
        for line in filtered_lines:
            if title in line and not found_title:
                found_title = True
                continue
            elif found_title:
                # å¦‚æœé‡åˆ°å¹´ä»½æ ‡è®°ï¼Œå¯èƒ½æ˜¯è¯—æ­Œç»“å°¾
                if re.match(r'^\d{4}\.\d{1,2}$', line):
                    poem_content.append(line)
                    break
                poem_content.append(line)
        
        return '\n'.join(poem_content[:30]) if poem_content else None  # é™åˆ¶é•¿åº¦
    
    def create_poem_markdown(self, poem_data, content):
        """åˆ›å»ºå•é¦–è¯—çš„markdownæ–‡ä»¶"""
        section_dir = self.sections[poem_data['section']]
        filename = f"{self.clean_filename(poem_data['title'])}.md"
        filepath = section_dir / filename
        
        # é¿å…æ–‡ä»¶åå†²çª
        counter = 1
        original_filepath = filepath
        while filepath.exists():
            name_without_ext = original_filepath.stem
            filepath = section_dir / f"{name_without_ext}-{counter}.md"
            counter += 1
        
        # åˆ›å»ºmarkdownå†…å®¹
        markdown_content = f"""# {poem_data['title']}

{content}

---

**åŸæ–‡é“¾æ¥**: {poem_data['url']}  
**ç« èŠ‚**: {self.get_section_name(poem_data['section'])}  
**è·¯å¾„**: {poem_data['path']}
"""
        
        # å†™å…¥æ–‡ä»¶
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            return str(filepath)
        except Exception as e:
            print(f"âŒ åˆ›å»ºæ–‡ä»¶å¤±è´¥ {filepath}: {e}")
            return None
    
    def get_section_name(self, section_key):
        """è·å–ç« èŠ‚ä¸­æ–‡åç§°"""
        section_names = {
            'preface': 'æ‰‰é¡µ',
            'part1': 'ç¬¬ä¸€ç¼–ã€€çŸ­è¯—ï¼ˆ1983â€”1986ï¼‰',
            'part2': 'ç¬¬äºŒç¼–ã€€é•¿è¯—ï¼ˆ1984â€”1985ï¼‰',
            'part3': 'ç¬¬ä¸‰ç¼–ã€€çŸ­è¯—ï¼ˆ1987â€”1989ï¼‰'
        }
        return section_names.get(section_key, 'æœªçŸ¥ç« èŠ‚')
    
    def scrape_all_poems(self):
        """çˆ¬å–æ‰€æœ‰è¯—æ­Œå¹¶ç”Ÿæˆç‹¬ç«‹æ–‡ä»¶"""
        print("ğŸš€ å¼€å§‹å…¨é‡çˆ¬å–æµ·å­è¯—é›†...")
        
        # è·å–æ‰€æœ‰è¯—æ­Œé“¾æ¥
        poem_links = self.extract_all_poem_links()
        if not poem_links:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°è¯—æ­Œé“¾æ¥")
            return
        
        total_poems = len(poem_links)
        print(f"ğŸ“ å‡†å¤‡çˆ¬å– {total_poems} é¦–è¯—æ­Œ...")
        
        for i, poem in enumerate(poem_links, 1):
            print(f"ğŸ“„ [{i:3d}/{total_poems}] å¤„ç†: {poem['title']}")
            
            # æå–è¯—æ­Œå†…å®¹
            content = self.extract_poem_content(poem['url'], poem['title'])
            
            if content:
                # åˆ›å»ºmarkdownæ–‡ä»¶
                filepath = self.create_poem_markdown(poem, content)
                if filepath:
                    self.scraped_poems.append({
                        'title': poem['title'],
                        'section': poem['section'],
                        'filepath': filepath,
                        'url': poem['url']
                    })
                    print(f"  âœ… å·²ä¿å­˜: {filepath}")
                else:
                    self.failed_poems.append(poem)
                    print(f"  âŒ ä¿å­˜å¤±è´¥: {poem['title']}")
            else:
                self.failed_poems.append(poem)
                print(f"  âŒ å†…å®¹æå–å¤±è´¥: {poem['title']}")
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
            time.sleep(0.2)
        
        self.create_summary()
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")
        print(f"âœ… æˆåŠŸ: {len(self.scraped_poems)} é¦–")
        print(f"âŒ å¤±è´¥: {len(self.failed_poems)} é¦–")
    
    def create_summary(self):
        """åˆ›å»ºçˆ¬å–æ€»ç»“æ–‡ä»¶"""
        summary_file = self.output_dir / "README.md"
        
        section_stats = {}
        for poem in self.scraped_poems:
            section = poem['section']
            if section not in section_stats:
                section_stats[section] = []
            section_stats[section].append(poem)
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("# æµ·å­è¯—é›† - çˆ¬å–æ€»ç»“\n\n")
            f.write(f"**çˆ¬å–æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**æ•°æ®æ¥æº**: {self.base_url}\n")
            f.write(f"**æˆåŠŸçˆ¬å–**: {len(self.scraped_poems)} é¦–è¯—æ­Œ\n")
            f.write(f"**å¤±è´¥æ•°é‡**: {len(self.failed_poems)} é¦–\n\n")
            
            f.write("## ç›®å½•ç»“æ„\n\n")
            f.write("```\n")
            f.write("poems/\n")
            for section_key, poems in section_stats.items():
                section_name = self.get_section_name(section_key)
                f.write(f"â”œâ”€â”€ {section_key}/ ({section_name})\n")
                f.write(f"â”‚   â””â”€â”€ {len(poems)} é¦–è¯—æ­Œ\n")
            f.write("```\n\n")
            
            f.write("## å„ç« èŠ‚è¯¦æƒ…\n\n")
            for section_key, poems in section_stats.items():
                section_name = self.get_section_name(section_key)
                f.write(f"### {section_name}\n\n")
                f.write(f"å…± {len(poems)} é¦–è¯—æ­Œï¼š\n\n")
                
                for poem in poems[:10]:  # åªæ˜¾ç¤ºå‰10é¦–
                    filename = Path(poem['filepath']).name
                    f.write(f"- [{poem['title']}]({section_key}/{filename})\n")
                
                if len(poems) > 10:
                    f.write(f"- ... è¿˜æœ‰ {len(poems) - 10} é¦–è¯—æ­Œ\n")
                f.write("\n")
            
            if self.failed_poems:
                f.write("## çˆ¬å–å¤±è´¥çš„è¯—æ­Œ\n\n")
                for poem in self.failed_poems:
                    f.write(f"- {poem['title']} ({poem['url']})\n")
        
        print(f"ğŸ“‹ æ€»ç»“æ–‡ä»¶å·²åˆ›å»º: {summary_file}")

if __name__ == "__main__":
    scraper = HaiziFullScraper()
    scraper.scrape_all_poems()